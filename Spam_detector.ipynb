{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spam Detector Demonstration (Sentiment Classification)\n",
    "This model performs spam detection on emails, classifying them as spam or not spam.\n",
    "Contact: rohan11parekh@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting Pytorch device\n",
    "device = torch.device(\"cuda\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the data from csv using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('datasets/Emails.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Body</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>\\nSave up to 70% on Life Insurance.\\nWhy Spend...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1) Fight The Risk of Cancer!\\nhttp://www.adcli...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1) Fight The Risk of Cancer!\\nhttp://www.adcli...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>##############################################...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>I thought you might like these:\\n1) Slim Down ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               Body  Label\n",
       "0           0  \\nSave up to 70% on Life Insurance.\\nWhy Spend...      1\n",
       "1           1  1) Fight The Risk of Cancer!\\nhttp://www.adcli...      1\n",
       "2           2  1) Fight The Risk of Cancer!\\nhttp://www.adcli...      1\n",
       "3           3  ##############################################...      1\n",
       "4           4  I thought you might like these:\\n1) Slim Down ...      1"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the ratio of spam to non-spam emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam: 1896\n",
      "Not spam: 4150\n"
     ]
    }
   ],
   "source": [
    "print(\"Spam:\", data['Label'].value_counts()[1] )\n",
    "print(\"Not spam:\", data['Label'].value_counts()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: 1 = spam, 0 = legit\n",
    "\n",
    "Now to clean the dataset of unused columns, null values, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Body</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>\\nSave up to 70% on Life Insurance.\\nWhy Spend...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1) Fight The Risk of Cancer!\\nhttp://www.adcli...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1) Fight The Risk of Cancer!\\nhttp://www.adcli...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>##############################################...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>I thought you might like these:\\n1) Slim Down ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               Body  Label\n",
       "0           0  \\nSave up to 70% on Life Insurance.\\nWhy Spend...      1\n",
       "1           1  1) Fight The Risk of Cancer!\\nhttp://www.adcli...      1\n",
       "2           2  1) Fight The Risk of Cancer!\\nhttp://www.adcli...      1\n",
       "3           3  ##############################################...      1\n",
       "4           4  I thought you might like these:\\n1) Slim Down ...      1"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_to_remove = 'empty'\n",
    "df = data[~data.apply(lambda row: value_to_remove in row.values, axis=1)]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Body</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1275</td>\n",
       "      <td>\\nUseful for your Individual and Business inve...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1860</td>\n",
       "      <td>WE NEED HELP.  We are a 14 year old fortune 50...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4981</td>\n",
       "      <td>\\nOh yeh one more thing.  None of this would e...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2617</td>\n",
       "      <td>&gt;\\n&gt; Sorry, Shrub, your political newspeak is ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4459</td>\n",
       "      <td>Hi,I wasn't sure if that ever started up.\\nwha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               Body  Label\n",
       "0        1275  \\nUseful for your Individual and Business inve...      1\n",
       "1        1860  WE NEED HELP.  We are a 14 year old fortune 50...      1\n",
       "2        4981  \\nOh yeh one more thing.  None of this would e...      0\n",
       "3        2617  >\\n> Sorry, Shrub, your political newspeak is ...      0\n",
       "4        4459  Hi,I wasn't sure if that ever started up.\\nwha...      0"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam: 1561\n",
      "Not spam: 3952\n"
     ]
    }
   ],
   "source": [
    "print(\"Spam:\", df['Label'].value_counts()[1])\n",
    "print(\"Not spam:\", df['Label'].value_counts()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nUseful for your Individual and Business investigation needs:\\nGET  THE TRUTH ABOUT ANYONE AND  ANYTHINGÂ… \\nover the internet\\n Please\\n  click on this link for more  information.\\nFinally find,\\n    track and learn  anything about\\n    anyone just like a professional  Private Eye, but without a license!  Find  \\npeople\\n    who have moved or changed their  name. Identify addresses, phone  numbers,\\n P.O. boxes. \\n Locate  \\nrelatives,\\n    old friends or your deadbeat spouse  you haven't seen in more than a  \\ndecade.\\nExercise\\n    your rights check public records and  obtain FBI,\\n    CIA and other government  documents.  Perform  \\ndo-it-yourself\\n    background checks as often as you  like, through the, Freedom of  \\nInformation\\n    Act. \\nCheck  the \\nlicenses,\\n    qualifications and disciplinary\\n    records of Doctors, Lawyers,  Accountants, Contractors. \\nDetect  \\nthe identity\\n    of birthparents or\\n    children given up for adoption as well  as their current whereabouts.\\nLearn  \\nabout\\n    Previous Lawsuits, Hidden  Assets, Concealed\\n    Ownership, Tax Liens, Court  Judgments, Criminal Records, Death,  \\nMarriage, Birth, Divorse Records and  MORE... Everyone needs a copy of this  program. It's like having a gateway to a  \\nvirtual\\n  investigative library through your own  computer.\\nWith\\n  the most talked about software on the  net...Internet Detective 7.5 \\nyou\\n  can verify anyone's income, assets,  education, employment history and  \\nbrushes\\n  with the law. Unearth your own family  secrets and check your credit report.  Track someone down from a phone  number, license plate, alias or s/s  \\nnumber,\\n  and much more... \\nPlease\\n  click on this link for more  information.\\n click here\\nto be removed from our mailing  list.\\n\""
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = df['Body'].to_list()\n",
    "emails = np.array(temp)\n",
    "emails[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining methods to clean emails of stopwords, punctuation, etc. and tokenizing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    text = text.encode('utf-8', 'ignore').decode('utf-8', 'ignore')\n",
    "\n",
    "    # Using RegEx to remove URLs, punctuation, newline characters\n",
    "    out = re.sub(r'(https?://\\S+|www\\.\\S+)|[^a-zA-Z\\s]', ' ', text)\n",
    "    out = out.lower()\n",
    "    out = \" \".join(out.split())\n",
    "    \n",
    "    # Tokenize and remove stop words\n",
    "    word_tokens = word_tokenize(out)\n",
    "    output = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "    return output\n",
    "\n",
    "def clean_list(s):\n",
    "    out = []\n",
    "    for item in s:\n",
    "        out.append(clean(item))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wed',\n",
       " 'aug',\n",
       " 'oates',\n",
       " 'isaac',\n",
       " 'wrote',\n",
       " 'new',\n",
       " 'razor',\n",
       " 'studied',\n",
       " 'trust',\n",
       " 'systems']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_emails = clean_list(emails)\n",
    "cleaned_emails[30][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a vocabulary of all words and their frequencies using Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(email_list):\n",
    "    count = Counter()\n",
    "    for email in email_list:\n",
    "        for word in email:\n",
    "            count[word.lower()] += 1\n",
    "    return count\n",
    "\n",
    "word_dict = count_words(cleaned_emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clearing variables for memory\n",
    "del stop_words\n",
    "del stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading pretrained word2vec embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "word_to_index = {\"<UNK>\": 0, **{word: idx + 1 for idx, word in enumerate(word_dict.keys())}}\n",
    "\n",
    "word2vec_path = 'GoogleNews-vectors-negative300.bin'\n",
    "word2vec = KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the embedding matrix with Word2Vec vectors or random vectors\n",
    "embedding_dim = 300 \n",
    "\n",
    "vocab_size = len(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60691"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embedding matrix with random values\n",
    "embedding_matrix = np.random.normal(0, 1, (vocab_size, embedding_dim))\n",
    "\n",
    "# Fill embedding matrix with Word2Vec embeddings\n",
    "for word, idx in word_to_index.items():\n",
    "    if word in word2vec:\n",
    "        embedding_matrix[idx] = word2vec[word]\n",
    "    else:\n",
    "        embedding_matrix[idx] = np.random.normal(0, 1, embedding_dim)  # For unknown words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert embedding matrix to PyTorch tensor\n",
    "embedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.3768e-01,  3.0560e-01,  8.3254e-01,  8.4590e-01,  7.6936e-01,\n",
       "         5.6505e-03,  5.1453e-01,  2.5337e-02, -7.4711e-01, -8.9757e-01,\n",
       "         9.2696e-01, -6.0390e-01,  4.3864e-01, -1.2269e+00,  1.3824e+00,\n",
       "        -2.4003e+00, -3.3544e-02, -1.5005e+00, -5.9447e-01, -3.2046e+00,\n",
       "        -2.0103e+00, -2.5262e-01,  1.4145e+00,  3.1132e-02,  4.6473e-01,\n",
       "         1.2684e+00, -1.7197e-01, -1.5119e-01, -8.5677e-01,  3.6958e-02,\n",
       "         1.3570e+00,  7.5293e-01, -1.4560e-01,  7.2415e-01,  4.7461e-03,\n",
       "        -3.0534e-01,  8.4862e-01, -2.5290e+00,  7.5127e-02, -8.4160e-01,\n",
       "        -8.6502e-01,  2.0582e-01,  8.6503e-01,  7.5544e-01, -1.2170e+00,\n",
       "         3.6528e-01,  8.0880e-02,  2.2561e-01, -6.1852e-01, -2.6137e+00,\n",
       "         2.4308e+00, -1.7626e-01, -8.7198e-01, -6.8907e-01, -2.0709e+00,\n",
       "        -6.6116e-01,  4.8864e-01, -9.3343e-01,  1.4454e+00, -5.3716e-01,\n",
       "         1.4327e+00, -1.3743e-01,  4.3624e-01,  4.3783e-01, -2.1283e-01,\n",
       "         7.1833e-01, -1.1722e+00,  9.1544e-01, -1.0086e+00,  1.9428e-01,\n",
       "        -3.4746e-02, -5.4892e-01, -4.4682e-01,  1.7787e-01, -4.3978e-01,\n",
       "         1.9500e+00, -9.0468e-04, -2.6397e-01, -9.5842e-01,  1.1516e-01,\n",
       "         9.7667e-01,  1.4588e+00, -1.1727e-01, -6.5865e-01, -1.0297e+00,\n",
       "         6.0771e-01, -1.3661e-01, -1.1430e+00,  1.1691e+00, -5.8214e-01,\n",
       "        -5.4431e-01, -1.0185e+00, -5.3629e-01,  5.7865e-01,  7.4171e-01,\n",
       "         8.1954e-02,  4.0380e-01, -1.4924e+00, -4.1658e-01,  5.1349e-01])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[0][0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accounting for unknown words\n",
    "from collections import defaultdict\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "vocab = defaultdict(lambda: len(vocab))\n",
    "UNK = vocab[\"<UNK>\"]\n",
    "\n",
    "for text in cleaned_emails:\n",
    "    for word in text:\n",
    "        _ = vocab[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the text needs to be converting into tensors so they can be fed into the model. To do this I define a method called text_to_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_tensor(text):\n",
    "    indices = [vocab.get(word, UNK) for word in text]  # Convert words to indices\n",
    "    return torch.tensor(indices, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,  14,\n",
       "         15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,   6,  26,  27,\n",
       "         28,  29,  30,   6,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,\n",
       "         41,  11,  42,  43,  44,  45,  46,  47,  48,  49,  50,  19,  51,  52,\n",
       "         42,  53,  54,  49,  15,  55,  56,  57,  58,  59,  60,  61,  62,  63,\n",
       "         64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,   3,   4,\n",
       "         76,  77,  65,  66,  78,  79,  80,  81,  82,  83,  75,  84,  85,  86,\n",
       "         87,  88,  89,  72,  73,   5,  90,  91,  92,  93,  94,  67,  76,  95,\n",
       "         91,  96,  97,  98,  99,   6, 100, 101, 102, 103, 104, 105, 106, 107,\n",
       "        108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121,\n",
       "        118, 122, 123, 124, 125, 126,  38, 127, 128, 129, 115, 130, 131, 132,\n",
       "        133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146,\n",
       "          4, 147, 148,   4, 149, 150, 134, 151, 152, 153, 154, 155, 156, 157,\n",
       "        145,   4, 158, 159, 160, 161, 162, 123,  98, 163, 164, 165, 166, 167,\n",
       "        100, 168, 118, 169, 170, 171,  98, 172, 173, 159, 174, 175, 176, 177,\n",
       "        178, 115, 179, 180, 181, 182,  50, 183, 184, 185, 186, 181, 187, 188,\n",
       "        189, 190,  64, 191, 191, 191,  43, 192,  64, 193, 194,  43, 195, 191,\n",
       "        191, 191, 191, 191, 191, 196, 197, 198, 199,  19, 200, 201, 202, 203,\n",
       "        204, 205, 206, 124, 207, 208, 209,   1, 210,   6, 211,  98, 212, 213,\n",
       "        214, 215, 216, 217, 218, 100, 219, 220, 221, 222, 223, 221, 224,   7,\n",
       "        225, 226, 227, 228, 229, 230, 231,  65, 232, 233, 234, 235, 236, 237,\n",
       "        238, 239, 240, 241, 242,   4, 243, 244, 245, 246, 247, 248, 249,  96,\n",
       "        250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261,  49, 262,\n",
       "        263])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_tensor = []\n",
    "for text in cleaned_emails:\n",
    "    temp_tensor.append(text_to_tensor(text))\n",
    "temp_tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding the train and test sets so all the inputs are equal length \n",
    "emails_pad = [(seq[:500]) for seq in temp_tensor]\n",
    "emails_pad = pad_sequence(emails_pad, batch_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clearing memory\n",
    "del cleaned_emails\n",
    "del data\n",
    "del temp\n",
    "del temp_tensor\n",
    "del word_to_index\n",
    "del vocab\n",
    "del text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5513"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(emails_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 1, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = np.array(df['Label'])\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5513"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is in a readable format, it can be split into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rohan Parekh\\AppData\\Local\\Temp\\ipykernel_3356\\3298461145.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(emails_pad[:5250])\n",
      "C:\\Users\\Rohan Parekh\\AppData\\Local\\Temp\\ipykernel_3356\\3298461145.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test = torch.tensor(emails_pad[5250:])\n"
     ]
    }
   ],
   "source": [
    "X_train = torch.tensor(emails_pad[:5250])\n",
    "X_test = torch.tensor(emails_pad[5250:])\n",
    "Y_train = torch.tensor(labels[:5250])\n",
    "Y_test = torch.tensor(labels[5250:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5250, 500])\n",
      "torch.Size([5250])\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,\n",
       "        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,\n",
       "        1, 0, 0, 1])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[100:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,\n",
       "       0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,\n",
       "       0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[100:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5513,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Label'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch Dataloaders\n",
    "import torch.utils.data as data_utils\n",
    "from torch.utils.data import DataLoader\n",
    "train = data_utils.TensorDataset(X_train, Y_train)\n",
    "train_loader = DataLoader(train, batch_size=16, shuffle=True)\n",
    "test = data_utils.TensorDataset(X_test, Y_test)\n",
    "test_loader = DataLoader(test, batch_size=16, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('list', 4431), ('one', 3907), ('e', 3779), ('get', 3697), ('email', 3585)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Listing the top 5 most common words\n",
    "word_dict.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = .01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the model using PyTorch, then training it for 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definition\n",
    "class SpamDetector(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpamDetector, self).__init__()\n",
    "        self.emb = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        self.LSTM = nn.LSTM(300, 200, batch_first=True)\n",
    "        self.LSTM2 = nn.LSTM(200, 300, batch_first=True)\n",
    "        self.fc1 = nn.Linear(300, 1000)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(1000, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        x, _ = self.LSTM(x)\n",
    "        x, _ = self.LSTM2(x)\n",
    "        x = x[:, -1, :]  # Taking the last hidden state\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x) \n",
    "        return x\n",
    "    # I don't return it with sigmoid because BCEWithLogitsLoss expects raw logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clearing memory\n",
    "del df\n",
    "del test\n",
    "del train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rohan Parekh\\AppData\\Local\\Temp\\ipykernel_3356\\1338083684.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  outputs = torch.tensor((outputs[:, 0] >= 0.5).float())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.0381, Accuracy: 69.49%\n",
      "Epoch [2/10], Loss: 0.9114, Accuracy: 72.82%\n",
      "Epoch [3/10], Loss: 1.0137, Accuracy: 71.26%\n",
      "Epoch [4/10], Loss: 0.2942, Accuracy: 94.99%\n",
      "Epoch [5/10], Loss: 0.1139, Accuracy: 98.21%\n",
      "Epoch [6/10], Loss: 0.0863, Accuracy: 98.74%\n",
      "Epoch [7/10], Loss: 0.3395, Accuracy: 97.31%\n",
      "Epoch [8/10], Loss: 1.1318, Accuracy: 95.12%\n",
      "Epoch [9/10], Loss: 0.5285, Accuracy: 97.14%\n",
      "Epoch [10/10], Loss: 0.2696, Accuracy: 98.55%\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Instantiate Model, Loss Function, and Optimizer\n",
    "model = SpamDetector().to(device)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.FloatTensor([4150/1896]).to(device)) \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "    accuracy = 0\n",
    "    avg_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        labels = labels.float()\n",
    "        # Forward Pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs[:,0], labels)\n",
    "        \n",
    "        # Backward and Optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Get predictions and compute accuracy\n",
    "        outputs = torch.tensor((outputs[:, 0] >= 0.5).float())\n",
    "        total += labels.size(0)  # Total number of labels\n",
    "        correct += (outputs == labels).sum().item()  # Count correct predictions\n",
    "        \n",
    "        avg_loss += loss.item()\n",
    "        \n",
    "    # Calculate and print the average loss and accuracy for the epoch\n",
    "    avg_loss /= len(train_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{NUM_EPOCHS}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running model on test set. I display the predictions to prove the results. Note: You will have to scroll a little bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Accuracy on test set: 98.83%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on Test Data\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # Move to device\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        labels = labels.float()\n",
    "        # Move tensors to CPU and convert to numpy arrays\n",
    "        outputs = outputs.cpu().numpy()\n",
    "        labels = labels.cpu().numpy()\n",
    "        \n",
    "        predictions = [1.0 if value >= 0.5 else 0.0 for value in outputs]\n",
    "\n",
    "        total += labels.shape[0]\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        \n",
    "        # Iterate over each sample in the batch\n",
    "        for i in range(len(predictions)):\n",
    "            print(f'Predicted: {predictions[i]} | Actual: {labels[i]}')\n",
    "\n",
    "print(f'Accuracy on test set: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displaying two individual results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do You Want To Make $1000 Or More Per Week? If you are a motivated and qualified individual - I \n",
      "will personally demonstrate to you a system that will \n",
      "make you $1,000 per week or more! This is NOT mlm. Call our 24 hour pre-recorded number to get the \n",
      "details.   801-296-4210 I need people who want to make serious money.  Make \n",
      "the call and get the facts. Invest 2 minutes in yourself now! 801-296-4210 Looking forward to your call and I will introduce you \n",
      "to people like yourself who\n",
      "are currently making $10,000 plus per week! 801-296-42103484lJGv6-241lEaN9080lRmS6-271WxHo7524qiyT5-438rjUv5615hQcf0-662eiDB9057dMtVl72\n",
      "\n",
      "Predicted class: 1.0\n",
      "Actual class:  1\n"
     ]
    }
   ],
   "source": [
    "emails_pad = emails_pad.to(device)\n",
    "with torch.no_grad():\n",
    "    out = model(emails_pad[12].unsqueeze(0))\n",
    "    predicted_class = torch.round(torch.sigmoid(out)) \n",
    "    print(emails[12]) # Printing first spam email without fishy links for safety\n",
    "    print(\"Predicted class:\", predicted_class.item())\n",
    "    print(\"Actual class: \", labels[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I will be out of the office starting  02/08/2002 and will not return until\n",
      "06/08/2002.I am out of the office until Tuesday 6th August.   I will reply to messages\n",
      "on my return.Thank you.\n",
      "DermotImportant Email InformationThe information in this email is confidential and may be legally\n",
      "privileged. It is intended solely for the addressee. Access to this email\n",
      "by anyone else is unauthorized. If you are not the intended recipient, any\n",
      "disclosure, copying, distribution or any action taken or omitted to be\n",
      "taken in reliance on it, is prohibited and may be unlawful. If you are not\n",
      "the intended addressee please contact the sender and dispose of this\n",
      "e-mail.-- \n",
      "Irish Linux Users' Group: ilug@linux.ie\n",
      "http://www.linux.ie/mailman/listinfo/ilug for (un)subscription information.\n",
      "List maintainer: listmaster@linux.ie\n",
      "\n",
      "Predicted class: 0.0\n",
      "Actual class:  0\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out = model(emails_pad[2].unsqueeze(0))\n",
    "    predicted_class = torch.round(torch.sigmoid(out))  \n",
    "    print(emails[2]) # First non-spam email \n",
    "    print(\"Predicted class:\", predicted_class.item())\n",
    "    print(\"Actual class: \", labels[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Email: rohan11parekh@gmail.com \n",
    "\n",
    "LinkedIn: https://www.linkedin.com/in/rohan-parekh-39b070225/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
