{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spam Detector Demonstration (Sentiment Classification)\n",
    "This model performs spam detection on emails, classifying them as spam or not spam.\n",
    "Contact: rohan11parekh@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting Pytorch device\n",
    "device = torch.device(\"cuda\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the data from csv using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('datasets/Emails.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Body</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>\\nSave up to 70% on Life Insurance.\\nWhy Spend...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1) Fight The Risk of Cancer!\\nhttp://www.adcli...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1) Fight The Risk of Cancer!\\nhttp://www.adcli...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>##############################################...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>I thought you might like these:\\n1) Slim Down ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               Body  Label\n",
       "0           0  \\nSave up to 70% on Life Insurance.\\nWhy Spend...      1\n",
       "1           1  1) Fight The Risk of Cancer!\\nhttp://www.adcli...      1\n",
       "2           2  1) Fight The Risk of Cancer!\\nhttp://www.adcli...      1\n",
       "3           3  ##############################################...      1\n",
       "4           4  I thought you might like these:\\n1) Slim Down ...      1"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the ratio of spam to non-spam emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam: 1896\n",
      "Not spam: 4150\n"
     ]
    }
   ],
   "source": [
    "print(\"Spam:\", data['Label'].value_counts()[1] )\n",
    "print(\"Not spam:\", data['Label'].value_counts()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: 1 = spam, 0 = legit\n",
    "\n",
    "Now to clean the dataset of unused columns, null values, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Body</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>\\nSave up to 70% on Life Insurance.\\nWhy Spend...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1) Fight The Risk of Cancer!\\nhttp://www.adcli...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1) Fight The Risk of Cancer!\\nhttp://www.adcli...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>##############################################...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>I thought you might like these:\\n1) Slim Down ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               Body  Label\n",
       "0           0  \\nSave up to 70% on Life Insurance.\\nWhy Spend...      1\n",
       "1           1  1) Fight The Risk of Cancer!\\nhttp://www.adcli...      1\n",
       "2           2  1) Fight The Risk of Cancer!\\nhttp://www.adcli...      1\n",
       "3           3  ##############################################...      1\n",
       "4           4  I thought you might like these:\\n1) Slim Down ...      1"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_to_remove = 'empty'\n",
    "df = data[~data.apply(lambda row: value_to_remove in row.values, axis=1)]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Body</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1275</td>\n",
       "      <td>\\nUseful for your Individual and Business inve...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1860</td>\n",
       "      <td>WE NEED HELP.  We are a 14 year old fortune 50...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4981</td>\n",
       "      <td>\\nOh yeh one more thing.  None of this would e...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2617</td>\n",
       "      <td>&gt;\\n&gt; Sorry, Shrub, your political newspeak is ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4459</td>\n",
       "      <td>Hi,I wasn't sure if that ever started up.\\nwha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               Body  Label\n",
       "0        1275  \\nUseful for your Individual and Business inve...      1\n",
       "1        1860  WE NEED HELP.  We are a 14 year old fortune 50...      1\n",
       "2        4981  \\nOh yeh one more thing.  None of this would e...      0\n",
       "3        2617  >\\n> Sorry, Shrub, your political newspeak is ...      0\n",
       "4        4459  Hi,I wasn't sure if that ever started up.\\nwha...      0"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam: 1561\n",
      "Not spam: 3952\n"
     ]
    }
   ],
   "source": [
    "print(\"Spam:\", df['Label'].value_counts()[1])\n",
    "print(\"Not spam:\", df['Label'].value_counts()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['use Perl Daily NewsletterIn this issue:\\n    * .NET and Perl, Working Together+--------------------------------------------------------------------+\\n| .NET and Perl, Working Together                                    |\\n|   posted by pudge on Tuesday August 27, @09:17 (links)             |\\n|   http://use.perl.org/article.pl?sid=02/08/27/1317253              |\\n+--------------------------------------------------------------------+[0]jonasbn writes \"DevX has brought an article on the subject of [1]Perl\\nand .NET and porting existing code. The teaser: Learn how CPAN Perl\\nmodules can be made automatically available to the .NET framework. The\\ntechnique involves providing small PerlNET mediators between Perl and\\n.NET and knowing when, where, and how to modify.\"Discuss this story at:\\n    http://use.perl.org/comments.pl?sid=02/08/27/1317253Links:\\n    0. mailto:jonasbn@io.dk\\n    1. http://www.devx.com/dotnet/articles/ym81502/ym81502-1.aspCopyright 1997-2002 pudge.  All rights reserved.\\n======================================================================You have received this message because you subscribed to it\\non use Perl.  To stop receiving this and other\\nmessages from use Perl, or to add more messages\\nor change your preferences, please go to your user page.\\thttp://use.perl.org/my/messages/You can log in and change your preferences from there.',\n",
       "       'URL: http://scriptingnews.userland.com/backissues/2002/09/25#When:8:47:05AM\\nDate: Wed, 25 Sep 2002 15:47:05 GMTKen Dow reports[1] that the current version of OmniOutliner can read and write \\nOPML. This means, for example, with a little Radio script (or an AppleScript) \\nyou could use Omni as an Instant Outliner[2].[1] http://radio.weblogs.com/0001015/images/2002/09/25/omniOutlinerWithOpml.jpg\\n[2] http://davenet.userland.com/2002/04/02/jonUdellOnInstantOutlining\\n',\n",
       "       'Untitled DocumentThis is what are customers are sayingâ€œI was shocked. I can\\'t believe how low the prices are, and the quality of the product is great. I will recommend your site  to everyone I know.â€\\x9d \\n----------------------------------------------\\n\"\\nCartridges were half of my lowest discount store prices. Delivery was in about 3-5  days.\\nCartridges work perfectly, so far. I will definitely order againâ€\\x9d   ------------------------------------------------------------\\nâ€œThe Product is excellent. Ink refill kits, easy to use and saving is substantial!â€\\x9d  â€œMy order came within three days - much faster than I expected. Definite savings over staples.\"\\n------------------------------------------------------------\\n\"Cheaper than any place else in town. Why buy new cartridges when you can get perfectly good results with refills for about 20% of original cost.â€\\x9d  \\nSavings are real. CLICK HERE\\nTo be removed from our mailing list click on the link below  and you will be removed from future mailings click here.\\n',\n",
       "       ...,\n",
       "       \"GT TONER\\nSUPPLIESLaser printer and computer supplies\\n  1-866-237-7397\\nSave up to 40% from retail price on laser printer toner\\ncartridges,\\ncopier and fax cartridges.\\n CHECK OUT OUR GREAT\\nPRICES!\\n \\nIf you received this email on error, please reply to gtts1@cable.net.co with subject: REMOVE... sorry for the inconvenience.\\nPlease forward to the person responsible for purchasing your laser printer\\nsupplies.Â\\xa0Â\\xa0Â\\xa0 Order by\\nphone: (toll free) 1-866-237-7397\\n \\nÂ\\xa0Â\\xa0Â\\xa0 Order by email: Simply reply this message or click hereÂ\\xa0\\nwith subject:  ORDER\\nÂ\\xa0Â\\xa0Â\\xa0    Email\\nremoval: Simply reply this message or click here  \\nÂ\\xa0 with subject:\\n REMOVEUniversity   and/orSchool   purchase\\norders WELCOME. (no credit approval required)\\nPay by check, c.o.d, or purchase order (net 30 days).\\nWE ACCEPT ALL MAJOR CREDIT CARDS!New! HP 4500/4550 series color\\ncartridges in stock!  Our cartridge prices are as follows: \\n(please order by item number) ItemÂ\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0\\nHewlett\\n     PackardÂ\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0\\nPrice 1 -- 92274A Toner Cartridge for LaserJet 4L, 4ML, 4P, 4MP\\n------------------------$47.502 -- C4092A Black Toner Cartridge for LaserJet 1100A, ASE, 3200SE-----------------$45.502A - C7115A Toner Cartridge For HP LaserJet 1000, 1200, 3330 ---------------------$55.502B - C7115X High Capacity Toner Cartridge for HP LaserJet 1000, 1200, 3330 -------$65.503 -- 92295A Toner Cartridge for LaserJet II, IID, III, IIID ----------------------$49.504 -- 92275A Toner Cartridge for LaserJet IIP, IIP+, IIIP -------------------------$55.505 -- C3903A Toner Cartridge for LaserJet 5P, 5MP, 6P, 6Pse, 6MP, 6Pxi ------------$46.506 -- C3909A Toner Cartridge for LaserJet 5Si, 5SiMX, 5Si Copier, 8000 ------------$92.507 -- C4096A Toner Cartridge for LaserJet 2100, 2200DSE, 2200DTN ------------------$72.508 - C4182X UltraPrecise High Capacity Toner Cartridge for LaserJet 8100 Series---$125.509 -- C3906A Toner Cartridge for LaserJet 5L, 5L Xtra, 6Lse, 6L, 6Lxi, 3100se------$42.50 9A - C3906A Toner Cartridge for LaserJet 3100, 3150 ------------------------------$42.5010 - C3900A Black Toner Cartridge for HP LaserJet 4MV, 4V ------------------------$89.5011 - C4127A Black Toner Cartridge for LaserJet 4000SE, 4000N, 4000T, 4000TN ------$76.5011A- C8061A Black Laser Toner for HP LaserJet 4100, 4100N ------------------------$76.5011B- C8061X High Capacity Toner Cartridge for LJ4100, 4100N ----------------------$85.5011C- C4127X High Capacity Black Cartridge for LaserJet 4000SE,4000N,4000T,4000TN\\n-$84.5012 - 92291A Toner Cartridge for LaserJet IIISi, 4Si, 4SiMX -----------------------$65.5013 - 92298A Toner Cartridge for LaserJet 4, 4 Plus, 4M, 4M Plus, 5, 5se, 5M, 5N\\n--$46.5014 - C4129X High Capacity Black Toner Cartridge for LaserJet 5000N ---------------$97.5015 - LASERFAX 500, 700 (FX1) -----------------------------------------------------$49.0016 - LASERFAX 5000, 7000 (FX2) ---------------------------------------------------$54.0017 - LASERFAX (FX3) --------------------------------------------------------------$49.0018 - LASERFAX (FX4) --------------------------------------------------------------$49.00 ItemÂ\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0\\nHewlett Packard - ColorÂ\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0\\nÂ\\xa0Â\\xa0\\nPrice C1 -- C4194a Toner\\nCartridge,\\nYellow (color lj 4500/4550 series)------------------ $ 89.50\\nC2 -- C4193a Toner\\nCartridge, Magenta (color lj 4500/4550 series)-----------------\\n$ 89.50\\nC3 -- C4192a toner cartridge, cyan (color lj 4500/4550 series)-------------------- $\\n89.50\\nC4 -- c4191a toner cartridge, black (color lj 4500/4550 series)-------------------\\n$ 74.50 ItemÂ\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0\\nLexmarkÂ\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0\\nPrice 19 - 1380520 High Yield Black Laser Toner for 4019, 4019E, 4028, 4029, 6, 10, 10L -- $109.50\\n20 - 1382150 High Yield Toner for 3112, 3116, 4039-10+, 4049- Model 12L,16R,\\nOptra - $109.50\\n21 - 69G8256 Laser Cartridge for Optra E,\\nE+, EP, ES, 4026, 4026 (6A,6B,6D,6E)Â\\xa0 ---- $ 49.00\\n22 - 13T0101 High Yield Toner Cartridge for Lexmark Optra E310, E312, E312L -------- $ 89.00\\n23 - 1382625 High-Yield Laser Toner Cartridge for Lexmark Optra S (4059) ----------- $129.50\\n24 - 12A5745 High Yield Laser Toner for Lexmark Optra T610, 612, 614 (4069) -------- $165.00 ItemÂ\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0\\nEpsonÂ\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0\\nPrice25\\n---- S051009 Toner Cartridge for Epson EPL7000, 7500, 8000+ - $115.50\\n25A --- S051009 LP-3000 PS 7000 -------------------------------- $115.50\\n26 ---- AS051011 Imaging Cartridge for\\nActionLaser-1000, 1500 -- $ 99.50\\n26A --- AS051011 EPL-5000, EPL-5100, EPL-5200 ------------------ $ 99.50ItemÂ\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0\\nPanasonicÂ\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0\\nPrice\\n27\\n------Nec series 2 models 90 and 95 ---------------------- $109.50\\nItemÂ\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0\\nÂ\\xa0Â\\xa0Â\\xa0Â\\xa0\\nAppleÂ\\xa0 Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0\\nPrice\\n28 ---- 2473G/A Laser Toner for LaserWriter Pro 600, 630, LaserWriter 16/600 PS -\\n$ 57.50\\n29 ---- 1960G/A Laser Toner for Apple LaserWriter Select, 300, 310, 360 --------- $\\n71.50\\n30 ---- M0089LL/A Toner Cartridge for Laserwriter 300, 320 (74A) ---------------- $\\n52.50\\n31 ---- M6002 Toner Cartridge for Laserwriter IINT, IINTX, IISC, IIF, IIG (95A) - $\\n47.50\\n31A --- M0089LL/A Toner Cartridge for Laserwriter\\nLS, NT, NTR, SC (75A) --------- $\\n55.50\\n32 ---- M4683G/A Laser Toner for LaserWriter 12, 640PS --------------------------\\n$ 85.50ItemÂ\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0\\nCanonÂ\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0\\nPrice\\n33 --- Fax\\nCFX-L3500, CFX-4000 CFX-L4500, CFX-L4500IE & IF FX3 ----------- $ 49.50\\n33A -- L-250, L-260i, L-300 FX3 ------------------------------------------\\n$ 49.50\\n33B -- LASER CLASS 2060, 2060P, 4000 FX3 ---------------------------------\\n$ 49.50\\n34 --- LASER CLASS 5000, 5500, 7000, 7100, 7500, 6000 FX2 ----------------\\n$ 49.50\\n35 --- FAX 5000 FX2 ------------------------------------------------------\\n$ 49.50\\n36 --- LASER CLASS 8500, 9000, 9000L, 9000MS, 9500, 9500 MS, 9500 S FX4 --\\n$ 49.50\\n36A -- Fax L700,720,760,770,775,777,780,785,790, & L3300 FX1\\n------------- $ 49.50\\n36B -- L-800, L-900 FX4 --------------------------------------------------\\n$ 49.50\\n37 --- A30R Toner Cartridge for PC-6, 6RE, 7, 11, 12 ---------------------\\n$ 59.50\\n38 --- E-40 Toner Cartridge for PC-720, 740, 770, 790,795, 920, 950, 980 -\\n$ 85.50\\n38A -- E-20 Toner Cartridge for PC-310, 325, 330, 330L, 400, 420, 430 ----\\n$ 85.50ItemÂ\\xa0Â\\xa0\\nXeroxÂ\\xa0Â\\xa0Â\\xa0 Price\\n39 ---- 6R900 75A ---- $ 55.50\\n40 ---- 6R903 98A ---- $ 46.50\\n41 ---- 6R902 95A ---- $ 49.50\\n42 ---- 6R901 91A ---- $ 65.50\\n43 ---- 6R908 06A ---- $ 42.50\\n44 ---- 6R899 74A ---- $ 47.50\\n45 ---- 6R928 96A ---- $ 72.50\\n46 ---- 6R926 27X ---- $ 84.50\\n47 ---- 6R906 09A ---- $ 92.50\\n48 ---- 6R907 4MV ---- $ 89.50\\n49 ---- 6R905 03A ---- $ 46.50\\n30 Day unlimited warranty included on all\\nproducts\\nGT Toner Supplies guarantees these cartridges to be free from defects in\\nworkmanship and material.\\nWe look\\nforward in doing business with you.CustomerÂ\\xa0\\nSatisfaction guaranteed\\nIf you are ordering by e-mail or\\nc.o.d. please fill out an order\\nform with the following information:Â\\xa0Â\\xa0Â\\xa0\\nÂ\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0\\nÂ\\xa0\\nphone number\\ncompany name\\nfirst and last name\\nstreet address\\ncity, state zip codeÂ\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0\\nOrder Now or\\ncall toll free  1-866-237-7397 If you are ordering by purchase order please fill out an order form\\nwith the following information:Â\\xa0Â\\xa0Â\\xa0 Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0purchase order number\\nphone number\\ncompany or school name\\nshipping address and billing address\\ncity, state zip codeÂ\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0Â\\xa0\\nOrder\\nNow\\nAll trade marks and brand names listed above are property\\nof the respective\\nholders and used for descriptive purposes only.--DeathToSpamDeathToSpamDeathToSpam--\\n-------------------------------------------------------\\nThis sf.net email is sponsored by: Jabber - The world's fastest growing \\nreal-time communications platform! Don't just IM. Build it in! \\nhttp://www.jabber.com/osdn/xim\\n_______________________________________________\\nSpamassassin-Sightings mailing list\\nSpamassassin-Sightings@lists.sourceforge.net\\nhttps://lists.sourceforge.net/lists/listinfo/spamassassin-sightings\\n\",\n",
       "       \"Quoting Greg Farrel (greg@netsoc.tcd.ie):> I'm looking to build a completely silent pc. It's gonna be a gateway for\\n> a wireless network and will sit in my room (as my room is only\\n> spitting distance from the chimney where i'll be mounting the aerial) \\n> It has to be completely silent, or it'll do my nut. What do I need? An old laptop would probably be ideal.  That way, it comes with its own \\nUPS (the battery).Some people strip down old 486 boxes, take out the hard drive, disable\\nthe fans, and run the thing from just a floppy drive or a CDR you've\\nburned for the purpose.  \\n-- \\nIrish Linux Users' Group Social Events: social@linux.ie\\nhttp://www.linux.ie/mailman/listinfo/social for (un)subscription information.\\nList maintainer: listmaster@linux.ie\\n\",\n",
       "       'WorldCom woes lead to bankruptcyÂ\\xa0Search\\n\\t                Â\\xa0\\n\\t                Â\\xa0\\nNews.com\\n\\t                All CNETÂ\\xa0Â\\xa0Â\\xa0Â\\xa0\\n\\t                The Web\\n                Â\\xa0Live tech help NOW!\\nApril\\'s tech award\\n1 million open jobs\\nNews.com: Top CIOs \\nZDNet: PeopleSoftJuly 22, 2002WorldCom woes lead to bankruptcyScripting flaw threatens Web serversCollabNet takes wing for online gamesRussian firm points out new Adobe flawBritish scientists finish Grid foundationApplied\\'s science: Building tinier chipsÂ\\xa0PerspectivesDo we need a national ID plan?\\nA White House proposal ignites a privacy firestorm.\\nRead Full Story\\nWorldCom woes lead to bankruptcyTelecommunications giant WorldCom files for bankruptcy, representing the largest such action in U.S. history, though it\\'s hardly alone. The company says it will emerge stronger; customers may get caught holding a dead line.\\nJuly 22, 2002, 8:00 AM PT\\n | Read Full Story Scripting flaw threatens Web serversA flaw found in newer versions of the PHP Web server scripting language could allow attackers to crash, and in some cases control, computers over the Internet, an open-source developer group announced Monday.\\nThe vulnerability affects versions 4.2.0 and 4.2.1 of PHP, according to the PHP Group. The flaw compromises different computer architectures in different ways: Web servers running on Intel IA-32 hardware could crash, while other systems, including Sun Microsystems\\' Solaris, could allow the attacker to infiltrate the computer.\\nJuly 22, 2002, 11:30 AM PT\\n | Read Full Story CollabNet takes wing for online gamesSoftware development site CollabNet announced Monday that it is working with Butterfly.net, an IBM-backed effort to create a supercomputing grid for hosting online games.\\nCollabNet will host the Butterfly Lab, an online environment that will allow developers to test games using a service modeled on the Butterfly approach. Butterfly will also use CollabBet\\'s SourceCast Web application to enable game development teams to share source code, manage art assets and design games for the Butterfly service.\\nJuly 22, 2002, 11:55 AM PT | Read Full Story \\nRussian firm points out new Adobe flawThumbing its nose at the company that landed one of its employees in jail, ElcomSoft is pointing out new flaws in Adobe Systems\\' eBook software.\\nThe flaws could allow someone to check out every copy of every book in Adobe\\'s new electronic library for an unlimited amount of time by changing the values in the loan form. However, the bugs were discovered on an Adobe test Web site that demonstrates how the software could be used to set up a lending library--not an actual site that offers books--and ElcomSoft gives information about how to fix the flaws.\\nJuly 22, 2002, 10:30 AM PT\\n | Read Full Story British scientists finish Grid foundationBritish scientists on Monday announced the completion of one of the building blocks for large-scale data sharing over the Grid--a proposed network of computers seen as the 21st century successor to the Internet.\\nThe project was carried out by scientists from the U.K.\\'s E-Science Centres and was co-funded by IBM and Oracle. The companies, rival powers in the database market, contributed both funding and the efforts of their own researchers.\\nJuly 22, 2002, 10:50 AM PT\\n | Read Full Story Applied\\'s science: Building tinier chipsApplied Materials is harnessing the atom to build better chips.\\nThe manufacturer of chipmaking equipment said Monday that its latest product will incorporate a technique called atomic layer deposition, which creates chips atom by atom.\\nJuly 22, 2002, 9:10 AM PT\\n | Read Full Story \\nFrom our partners:Nvidia: Dodging a hail of bulletsBusiness Week\\nCan the chip wizard fight off rivals and the SEC?\\nJuly 29, 2002 issue\\n | Read Full Story Sohu: So far, so good--sort ofBusiness Week\\nThe Chinese portal should go easy celebrating its first quarterly profit. It still faces a chilling threat from Beijing\\'s thought police.July 22, 2002\\n | Read Full Story \\nAlso from CNET:Real-time stock quotes from CNET News.com Investor.30-day free trial!\\nDigicams for summer shutterbugsGoing on vacation, or just headed to the beach? Indulge your summer snapshot habit with one of our picks.\\nâ€¢ 5-megapixel shoot-out\\nâ€¢ Leica Digilux 1: street shooter\"s digicamMost popular products\\nDigital cameras\\n1. Canon PowerShot G2\\n2. Canon PowerShot S40\\n3. Canon PowerShot S30\\n4. Canon PowerShot A40\\n5. Nikon Coolpix 995\\n See all most popular cameras\\nÂ\\xa0Apple developers take center stage\\nAt Macworld in New York, ZDNet Anchordesk Executive Editor David Coursey looks at the latest applications and hardware coming from Apple developers, including a new personal video recorder and back-up storage drive.\\n Watch Video\\nÂ\\xa0Â\\xa0EnterpriseGateway to unveil new ProfileGroove tightens ties to MicrosoftCheck Point\\'s revenue plummetsE-BusinessMarkets tumble on earnings outlooksPayPal, Stamps.com seal dealBig chill for federal IT spendingCommunicationsSatellite phones getting taste of cellularWorldCom seeks OK to pay severanceBellSouth shares skid on earnings woesMediaReal takes the open source routeIs Internet radio doomed?Ogg Vorbis official release is herePersonal TechnologyHP seeks Texas justice; Gateway gets dingedLevel 3 to help power Xbox LiveLexmark earnings surprise Wall Street        Â\\xa0\\nThe e-mail address for your subscription isÂ\\xa0qqqqqqqqqq-zdnet@spamassassin.taint.org\\nUnsubscribeÂ\\xa0|Â\\xa0Manage My SubscriptionsÂ\\xa0|Â\\xa0FAQÂ\\xa0|Â\\xa0AdvertisePlease send any questions, comments, or concerns toÂ\\xa0dispatchfeedback@news.com.Price comparisons |\\nProduct reviews |\\nTech news |\\nDownloads |\\nAll CNET services        Copyright 2002 CNET Networks, Inc. All rights reserved.          '],\n",
       "      dtype='<U194978')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = df['Body'].to_list()\n",
    "emails = np.array(temp)\n",
    "emails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining methods to clean emails of stopwords, punctuation, etc. and tokenizing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    text = text.encode('utf-8', 'ignore').decode('utf-8', 'ignore')\n",
    "\n",
    "    # Using RegEx to remove URLs, punctuation, newline characters\n",
    "    out = re.sub(r'(https?://\\S+|www\\.\\S+)|[^a-zA-Z\\s]', ' ', text)\n",
    "    out = out.lower()\n",
    "    out = \" \".join(out.split())\n",
    "    \n",
    "    # Tokenize and remove stop words\n",
    "    word_tokens = word_tokenize(out)\n",
    "    output = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "    return output\n",
    "\n",
    "def clean_list(s):\n",
    "    out = []\n",
    "    for item in s:\n",
    "        out.append(clean(item))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wed',\n",
       " 'aug',\n",
       " 'oates',\n",
       " 'isaac',\n",
       " 'wrote',\n",
       " 'new',\n",
       " 'razor',\n",
       " 'studied',\n",
       " 'trust',\n",
       " 'systems']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_emails = clean_list(emails)\n",
    "cleaned_emails[30][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a vocabulary of all words and their frequencies using Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(email_list):\n",
    "    count = Counter()\n",
    "    for email in email_list:\n",
    "        for word in email:\n",
    "            count[word.lower()] += 1\n",
    "    return count\n",
    "\n",
    "word_dict = count_words(cleaned_emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clearing variables for memory\n",
    "del stop_words\n",
    "del stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading pretrained word2vec embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "word_to_index = {\"<UNK>\": 0, **{word: idx + 1 for idx, word in enumerate(word_dict.keys())}}\n",
    "\n",
    "word2vec_path = 'GoogleNews-vectors-negative300.bin'\n",
    "word2vec = KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the embedding matrix with Word2Vec vectors or random vectors\n",
    "embedding_dim = 300 \n",
    "\n",
    "vocab_size = len(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60691"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embedding matrix with random values\n",
    "embedding_matrix = np.random.normal(0, 1, (vocab_size, embedding_dim))\n",
    "\n",
    "# Fill embedding matrix with Word2Vec embeddings\n",
    "for word, idx in word_to_index.items():\n",
    "    if word in word2vec:\n",
    "        embedding_matrix[idx] = word2vec[word]\n",
    "    else:\n",
    "        embedding_matrix[idx] = np.random.normal(0, 1, embedding_dim)  # For unknown words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert embedding matrix to PyTorch tensor\n",
    "embedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.3768e-01,  3.0560e-01,  8.3254e-01,  8.4590e-01,  7.6936e-01,\n",
       "         5.6505e-03,  5.1453e-01,  2.5337e-02, -7.4711e-01, -8.9757e-01,\n",
       "         9.2696e-01, -6.0390e-01,  4.3864e-01, -1.2269e+00,  1.3824e+00,\n",
       "        -2.4003e+00, -3.3544e-02, -1.5005e+00, -5.9447e-01, -3.2046e+00,\n",
       "        -2.0103e+00, -2.5262e-01,  1.4145e+00,  3.1132e-02,  4.6473e-01,\n",
       "         1.2684e+00, -1.7197e-01, -1.5119e-01, -8.5677e-01,  3.6958e-02,\n",
       "         1.3570e+00,  7.5293e-01, -1.4560e-01,  7.2415e-01,  4.7461e-03,\n",
       "        -3.0534e-01,  8.4862e-01, -2.5290e+00,  7.5127e-02, -8.4160e-01,\n",
       "        -8.6502e-01,  2.0582e-01,  8.6503e-01,  7.5544e-01, -1.2170e+00,\n",
       "         3.6528e-01,  8.0880e-02,  2.2561e-01, -6.1852e-01, -2.6137e+00,\n",
       "         2.4308e+00, -1.7626e-01, -8.7198e-01, -6.8907e-01, -2.0709e+00,\n",
       "        -6.6116e-01,  4.8864e-01, -9.3343e-01,  1.4454e+00, -5.3716e-01,\n",
       "         1.4327e+00, -1.3743e-01,  4.3624e-01,  4.3783e-01, -2.1283e-01,\n",
       "         7.1833e-01, -1.1722e+00,  9.1544e-01, -1.0086e+00,  1.9428e-01,\n",
       "        -3.4746e-02, -5.4892e-01, -4.4682e-01,  1.7787e-01, -4.3978e-01,\n",
       "         1.9500e+00, -9.0468e-04, -2.6397e-01, -9.5842e-01,  1.1516e-01,\n",
       "         9.7667e-01,  1.4588e+00, -1.1727e-01, -6.5865e-01, -1.0297e+00,\n",
       "         6.0771e-01, -1.3661e-01, -1.1430e+00,  1.1691e+00, -5.8214e-01,\n",
       "        -5.4431e-01, -1.0185e+00, -5.3629e-01,  5.7865e-01,  7.4171e-01,\n",
       "         8.1954e-02,  4.0380e-01, -1.4924e+00, -4.1658e-01,  5.1349e-01])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[0][0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accounting for unknown words\n",
    "from collections import defaultdict\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "vocab = defaultdict(lambda: len(vocab))\n",
    "UNK = vocab[\"<UNK>\"]\n",
    "\n",
    "for text in cleaned_emails:\n",
    "    for word in text:\n",
    "        _ = vocab[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the text needs to be converting into tensors so they can be fed into the model. To do this I define a method called text_to_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_tensor(text):\n",
    "    indices = [vocab.get(word, UNK) for word in text]  # Convert words to indices\n",
    "    return torch.tensor(indices, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,  14,\n",
       "         15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,   6,  26,  27,\n",
       "         28,  29,  30,   6,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,\n",
       "         41,  11,  42,  43,  44,  45,  46,  47,  48,  49,  50,  19,  51,  52,\n",
       "         42,  53,  54,  49,  15,  55,  56,  57,  58,  59,  60,  61,  62,  63,\n",
       "         64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,   3,   4,\n",
       "         76,  77,  65,  66,  78,  79,  80,  81,  82,  83,  75,  84,  85,  86,\n",
       "         87,  88,  89,  72,  73,   5,  90,  91,  92,  93,  94,  67,  76,  95,\n",
       "         91,  96,  97,  98,  99,   6, 100, 101, 102, 103, 104, 105, 106, 107,\n",
       "        108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121,\n",
       "        118, 122, 123, 124, 125, 126,  38, 127, 128, 129, 115, 130, 131, 132,\n",
       "        133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146,\n",
       "          4, 147, 148,   4, 149, 150, 134, 151, 152, 153, 154, 155, 156, 157,\n",
       "        145,   4, 158, 159, 160, 161, 162, 123,  98, 163, 164, 165, 166, 167,\n",
       "        100, 168, 118, 169, 170, 171,  98, 172, 173, 159, 174, 175, 176, 177,\n",
       "        178, 115, 179, 180, 181, 182,  50, 183, 184, 185, 186, 181, 187, 188,\n",
       "        189, 190,  64, 191, 191, 191,  43, 192,  64, 193, 194,  43, 195, 191,\n",
       "        191, 191, 191, 191, 191, 196, 197, 198, 199,  19, 200, 201, 202, 203,\n",
       "        204, 205, 206, 124, 207, 208, 209,   1, 210,   6, 211,  98, 212, 213,\n",
       "        214, 215, 216, 217, 218, 100, 219, 220, 221, 222, 223, 221, 224,   7,\n",
       "        225, 226, 227, 228, 229, 230, 231,  65, 232, 233, 234, 235, 236, 237,\n",
       "        238, 239, 240, 241, 242,   4, 243, 244, 245, 246, 247, 248, 249,  96,\n",
       "        250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261,  49, 262,\n",
       "        263])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_tensor = []\n",
    "for text in cleaned_emails:\n",
    "    temp_tensor.append(text_to_tensor(text))\n",
    "temp_tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding the train and test sets so all the inputs are equal length \n",
    "emails_pad = [(seq[:500]) for seq in temp_tensor]\n",
    "emails_pad = pad_sequence(emails_pad, batch_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clearing memory\n",
    "del cleaned_emails\n",
    "del data\n",
    "del temp\n",
    "del temp_tensor\n",
    "del word_to_index\n",
    "del vocab\n",
    "del text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5513"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(emails_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 1, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = np.array(df['Label'])\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5513"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is in a readable format, it can be split into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rohan Parekh\\AppData\\Local\\Temp\\ipykernel_3356\\3298461145.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(emails_pad[:5250])\n",
      "C:\\Users\\Rohan Parekh\\AppData\\Local\\Temp\\ipykernel_3356\\3298461145.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test = torch.tensor(emails_pad[5250:])\n"
     ]
    }
   ],
   "source": [
    "X_train = torch.tensor(emails_pad[:5250])\n",
    "X_test = torch.tensor(emails_pad[5250:])\n",
    "Y_train = torch.tensor(labels[:5250])\n",
    "Y_test = torch.tensor(labels[5250:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5250, 500])\n",
      "torch.Size([5250])\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,\n",
       "        0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,\n",
       "        1, 0, 0, 1])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[100:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,\n",
       "       0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,\n",
       "       0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[100:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5513,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Label'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch Dataloaders\n",
    "import torch.utils.data as data_utils\n",
    "from torch.utils.data import DataLoader\n",
    "train = data_utils.TensorDataset(X_train, Y_train)\n",
    "train_loader = DataLoader(train, batch_size=16, shuffle=True)\n",
    "test = data_utils.TensorDataset(X_test, Y_test)\n",
    "test_loader = DataLoader(test, batch_size=16, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('list', 4431), ('one', 3907), ('e', 3779), ('get', 3697), ('email', 3585)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Listing the top 5 most common words\n",
    "word_dict.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = .01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the model using PyTorch, then training it for 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definition\n",
    "class SpamDetector(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpamDetector, self).__init__()\n",
    "        self.emb = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        self.LSTM = nn.LSTM(300, 200, batch_first=True)\n",
    "        self.LSTM2 = nn.LSTM(200, 300, batch_first=True)\n",
    "        self.fc1 = nn.Linear(300, 1000)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(1000, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        x, _ = self.LSTM(x)\n",
    "        x, _ = self.LSTM2(x)\n",
    "        x = x[:, -1, :]  # Taking the last hidden state\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x) \n",
    "        return x\n",
    "    # I don't return it with sigmoid because BCEWithLogitsLoss expects raw logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clearing memory\n",
    "del df\n",
    "del test\n",
    "del train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rohan Parekh\\AppData\\Local\\Temp\\ipykernel_3356\\1338083684.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  outputs = torch.tensor((outputs[:, 0] >= 0.5).float())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.0381, Accuracy: 69.49%\n",
      "Epoch [2/10], Loss: 0.9114, Accuracy: 72.82%\n",
      "Epoch [3/10], Loss: 1.0137, Accuracy: 71.26%\n",
      "Epoch [4/10], Loss: 0.2942, Accuracy: 94.99%\n",
      "Epoch [5/10], Loss: 0.1139, Accuracy: 98.21%\n",
      "Epoch [6/10], Loss: 0.0863, Accuracy: 98.74%\n",
      "Epoch [7/10], Loss: 0.3395, Accuracy: 97.31%\n",
      "Epoch [8/10], Loss: 1.1318, Accuracy: 95.12%\n",
      "Epoch [9/10], Loss: 0.5285, Accuracy: 97.14%\n",
      "Epoch [10/10], Loss: 0.2696, Accuracy: 98.55%\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Instantiate Model, Loss Function, and Optimizer\n",
    "model = SpamDetector().to(device)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.FloatTensor([4150/1896]).to(device)) \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "    accuracy = 0\n",
    "    avg_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        labels = labels.float()\n",
    "        # Forward Pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs[:,0], labels)\n",
    "        \n",
    "        # Backward and Optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Get predictions and compute accuracy\n",
    "        outputs = torch.tensor((outputs[:, 0] >= 0.5).float())\n",
    "        total += labels.size(0)  # Total number of labels\n",
    "        correct += (outputs == labels).sum().item()  # Count correct predictions\n",
    "        \n",
    "        avg_loss += loss.item()\n",
    "        \n",
    "    # Calculate and print the average loss and accuracy for the epoch\n",
    "    avg_loss /= len(train_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{NUM_EPOCHS}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 0.0 | Actual: 0.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Predicted: 1.0 | Actual: 1.0\n",
      "Accuracy on test set: 98.83%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on Test Data\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # Move to device\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        labels = labels.float()\n",
    "        # Move tensors to CPU and convert to numpy arrays\n",
    "        outputs = outputs.cpu().numpy()\n",
    "        labels = labels.cpu().numpy()\n",
    "        \n",
    "        predictions = [1.0 if value >= 0.5 else 0.0 for value in outputs]\n",
    "\n",
    "        total += labels.shape[0]\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        \n",
    "        # Iterate over each sample in the batch\n",
    "        for i in range(len(predictions)):\n",
    "            print(f'Predicted: {predictions[i]} | Actual: {labels[i]}')\n",
    "\n",
    "print(f'Accuracy on test set: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displaying two individual results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do You Want To Make $1000 Or More Per Week? If you are a motivated and qualified individual - I \n",
      "will personally demonstrate to you a system that will \n",
      "make you $1,000 per week or more! This is NOT mlm. Call our 24 hour pre-recorded number to get the \n",
      "details.   801-296-4210 I need people who want to make serious money.  Make \n",
      "the call and get the facts. Invest 2 minutes in yourself now! 801-296-4210 Looking forward to your call and I will introduce you \n",
      "to people like yourself who\n",
      "are currently making $10,000 plus per week! 801-296-42103484lJGv6-241lEaN9080lRmS6-271WxHo7524qiyT5-438rjUv5615hQcf0-662eiDB9057dMtVl72\n",
      "\n",
      "Predicted class: 1.0\n",
      "Actual class:  1\n"
     ]
    }
   ],
   "source": [
    "emails_pad = emails_pad.to(device)\n",
    "with torch.no_grad():\n",
    "    out = model(emails_pad[12].unsqueeze(0))\n",
    "    predicted_class = torch.round(torch.sigmoid(out)) \n",
    "    print(emails[12]) # Printing first spam email without fishy links for safety\n",
    "    print(\"Predicted class:\", predicted_class.item())\n",
    "    print(\"Actual class: \", labels[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I will be out of the office starting  02/08/2002 and will not return until\n",
      "06/08/2002.I am out of the office until Tuesday 6th August.   I will reply to messages\n",
      "on my return.Thank you.\n",
      "DermotImportant Email InformationThe information in this email is confidential and may be legally\n",
      "privileged. It is intended solely for the addressee. Access to this email\n",
      "by anyone else is unauthorized. If you are not the intended recipient, any\n",
      "disclosure, copying, distribution or any action taken or omitted to be\n",
      "taken in reliance on it, is prohibited and may be unlawful. If you are not\n",
      "the intended addressee please contact the sender and dispose of this\n",
      "e-mail.-- \n",
      "Irish Linux Users' Group: ilug@linux.ie\n",
      "http://www.linux.ie/mailman/listinfo/ilug for (un)subscription information.\n",
      "List maintainer: listmaster@linux.ie\n",
      "\n",
      "Predicted class: 0.0\n",
      "Actual class:  0\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out = model(emails_pad[2].unsqueeze(0))\n",
    "    predicted_class = torch.round(torch.sigmoid(out))  \n",
    "    print(emails[2]) # First non-spam email \n",
    "    print(\"Predicted class:\", predicted_class.item())\n",
    "    print(\"Actual class: \", labels[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Email: rohan11parekh@gmail.com \n",
    "\n",
    "LinkedIn: https://www.linkedin.com/in/rohan-parekh-39b070225/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
